# üî¨ Diagn√≥stico: Problema en la Extracci√≥n de Noticias WSJ

**Fecha:** 2 de diciembre de 2025  
**Analista:** Sistema de an√°lisis automatizado  
**Archivos analizados:** `hipervinculos_wsj.csv`, `articulos_filtrados_ordenados.csv`

---

## üìã Resumen Ejecutivo

**HALLAZGO PRINCIPAL:** El script de web scraping del Wall Street Journal cambi√≥ su 
m√©todo de extracci√≥n entre agosto 2020 y enero 2021, causando una ca√≠da del **94%** 
en el volumen de art√≠culos capturados.

### ‚ö†Ô∏è Severidad del Problema

- **Impacto:** CR√çTICO
- **Per√≠odo afectado:** Agosto 2020 - Presente (2025)
- **Datos perdidos:** ~150,000 art√≠culos estimados
- **Sesgo resultante:** 95.8% del corpus final proviene de 2016-2020

---

## üîç An√°lisis Detallado

### 1. Evidencia del Cambio en el M√©todo

#### **Per√≠odo Pre-Cambio (2016 - Julio 2020)**

```
M√©todo: Extracci√≥n selectiva de art√≠culos
Precisi√≥n: 99% art√≠culos reales
Patr√≥n: Solo URLs con /articles/...-[n√∫meros]

Volumen mensual promedio: 2,000 art√≠culos/mes
Total extra√≠do: ~162,000 art√≠culos
```

#### **Transici√≥n (Agosto 2020 - Diciembre 2020)**

```
Evento: Cambio en el script de scraping
Ca√≠da mensual: De 1,718 (julio) ‚Üí 454 (octubre)
Precisi√≥n degradada: 78% art√≠culos reales
```

#### **Per√≠odo Post-Cambio (2021 - 2025)**

```
M√©todo: Extracci√≥n indiscriminada de todos los hiperv√≠nculos
Precisi√≥n: 0.2% - 11% art√≠culos reales
Contaminaci√≥n: 89-99% URLs de navegaci√≥n/men√∫s

Volumen mensual promedio: 180 art√≠culos/mes
Total extra√≠do: ~11,000 art√≠culos
```

---

## üìä Datos Cuantitativos

### Composici√≥n del Dataset Original

| Archivo | Registros | Art√≠culos Reales | URLs Navegaci√≥n | % Precisi√≥n |
|---------|-----------|------------------|-----------------|-------------|
| `hipervinculos_wsj.csv` | 294,610 | 188,469 | 106,141 | 63.97% |

### Distribuci√≥n Temporal de Art√≠culos Reales

| A√±o  | Total Registros | Art√≠culos | % Art√≠culos | Estado |
|------|----------------|-----------|-------------|---------|
| 2016 | 58,037 | 57,365 | 98.84% | ‚úÖ Excelente |
| 2017 | 45,613 | 45,308 | 99.33% | ‚úÖ Excelente |
| 2018 | 36,245 | 35,925 | 99.12% | ‚úÖ Excelente |
| 2019 | 23,420 | 23,142 | 98.81% | ‚úÖ Excelente |
| 2020 | 20,437 | 15,972 | 78.15% | ‚ö†Ô∏è Degradado |
| 2021 | 20,109 | 2,139 | 10.64% | ‚ùå Malo |
| 2022 | 19,303 | 6,424 | 33.28% | ‚ùå Malo |
| 2023 | 18,926 | 2,099 | 11.09% | ‚ùå Malo |
| 2024 | 24,736 | 48 | 0.19% | üî¥ Cr√≠tico |
| 2025 | 27,784 | 47 | 0.17% | üî¥ Cr√≠tico |

### Evoluci√≥n Mensual del Punto de Quiebre (2020)

```
Mes       Art√≠culos    Cambio
-------------------------------
Enero     2,024       (baseline)
Febrero   1,821       -10%
Marzo     1,885       +4%
Abril     1,851       -2%
Mayo      1,976       +7%
Junio     2,047       +4%
Julio     1,718       -16%
Agosto    1,150       -33%  ‚ö†Ô∏è INICIO DEL PROBLEMA
Sept      751         -35%
Octubre   454         -40%
Nov       125         -72%
Diciembre 170         -26%
```

---

## üéØ Causa Ra√≠z

### Hip√≥tesis Principal: Cambio en la L√≥gica de Scraping

**Comportamiento Antiguo (2016-2019):**
```python
# Pseudoc√≥digo del m√©todo original
for page in wsj_pages:
    articles = page.find_all('article')  # Solo art√≠culos
    for article in articles:
        url = article.find('a')['href']
        if matches_pattern(url, r'/articles/.*-\d+$'):
            save_to_csv(url)
```

**Comportamiento Nuevo (2020+):**
```python
# Pseudoc√≥digo del m√©todo defectuoso
for page in wsj_pages:
    all_links = page.find_all('a')  # TODOS los enlaces
    for link in all_links:
        url = link['href']
        save_to_csv(url)  # No filtra por tipo
```

### Consecuencias:

1. **Contaminaci√≥n masiva:** 36% del dataset son URLs in√∫tiles
   - Men√∫s de navegaci√≥n (`/world`, `/business`, `/tech`)
   - Enlaces a secciones (`/news/archive/`)
   - Videos y multimedia (`/video/series/`)
   - P√°ginas de login/logout

2. **P√©rdida de cobertura:** En lugar de capturar m√°s art√≠culos, 
   captura menos porque el script se distrae con navegaci√≥n

3. **Inconsistencia temporal:** Dataset no comparable entre per√≠odos

---

## ‚úÖ Validaci√≥n del Post-Procesamiento

### Archivo: `filtrado_noticias.py`

**Estado:** ‚úÖ FUNCIONA CORRECTAMENTE

```python
# Filtro aplicado
filtro = df['url'].str.contains(r'/articles/.*-\d+$', regex=True)
```

**Resultados:**
- ‚úÖ Elimina 106,141 URLs in√∫tiles (36% del total)
- ‚úÖ Retiene 188,469 art√≠culos reales (64%)
- ‚úÖ Patr√≥n regex es correcto y preciso
- ‚úÖ Todos los t√≠tulos con "gold" pasan el filtro

**Conclusi√≥n:** El problema NO est√° en el post-procesamiento, 
sino en la **fase de extracci√≥n original**.

---

## üõ†Ô∏è Soluciones Propuestas

### Opci√≥n 1: Corto Plazo (IMPLEMENTADA)

**Estado:** ‚úÖ Completado

- [x] Documentar limitaci√≥n en metodolog√≠a
- [x] Actualizar informe cient√≠fico con secci√≥n de limitaciones
- [x] An√°lisis ponderado por densidad temporal
- [x] Conclusiones basadas principalmente en 2016-2020

**Ventajas:**
- No requiere trabajo adicional
- Resultados actuales son v√°lidos con caveat documentado

**Desventajas:**
- Dataset sigue incompleto para a√±os recientes
- An√°lisis sesgado hacia per√≠odo pre-2021

---

### Opci√≥n 2: Largo Plazo (RECOMENDADA)

**Estado:** ‚è≥ Pendiente

1. **Identificar script original de scraping**
   - Buscar en repositorio/historial
   - Contactar con quien desarroll√≥ el scraping original

2. **Corregir m√©todo de extracci√≥n**
   ```python
   # Implementaci√≥n correcta
   def scrape_wsj_articles(date):
       page = fetch_wsj_archive(date)
       articles = page.select('article.WSJTheme--story')  # Solo art√≠culos
       for article in articles:
           url = article.select_one('a')['href']
           if '/articles/' in url and url.endswith(tuple('0123456789')):
               title = article.select_one('h3').text
               save_article(title, url, date)
   ```

3. **Re-ejecutar scraping para 2020-2025**
   - Validar que capture >95% art√≠culos reales
   - Comparar con muestra existente para verificar consistencia

4. **Integrar nuevos datos**
   - Merge con dataset existente
   - Verificar no hay duplicados
   - Re-ejecutar an√°lisis completo

**Esfuerzo estimado:** 40-80 horas
- 8h: An√°lisis de script original
- 16h: Correcci√≥n y testing
- 24h: Re-scraping (depende de velocidad)
- 8h: Integraci√≥n y validaci√≥n
- 8h: Re-ejecuci√≥n de an√°lisis

**Ventajas:**
- Dataset completo y balanceado
- An√°lisis m√°s robusto para todo el per√≠odo
- Elimina sesgo temporal

**Desventajas:**
- Requiere tiempo significativo
- Puede enfrentar restricciones de acceso WSJ
- Riesgo de rate limiting/bloqueo

---

### Opci√≥n 3: Alternativa (EXPLORATORIA)

**Estado:** üí° Propuesta

**Complementar con fuentes adicionales:**

1. **APIs de noticias financieras:**
   - NewsAPI.org (40,000 art√≠culos/mes gratis)
   - GDELT Project (archivo global de noticias)
   - Bloomberg API (requiere suscripci√≥n)
   - Alpha Vantage News (gratis, limitado)

2. **Implementaci√≥n:**
   ```python
   # Ejemplo con NewsAPI
   from newsapi import NewsApiClient
   
   newsapi = NewsApiClient(api_key='YOUR_KEY')
   articles = newsapi.get_everything(
       q='gold OR "gold price" OR bullion',
       sources='the-wall-street-journal',
       from_param='2021-01-01',
       to='2025-12-31',
       language='en'
   )
   ```

3. **Validaci√≥n cruzada:**
   - Comparar art√≠culos de API vs WSJ scrapeado
   - Verificar overlap para per√≠odo 2016-2020
   - Validar consistencia de an√°lisis de sentimientos

**Ventajas:**
- Datos estructurados y limpios
- Cobertura temporal completa
- Mayor diversidad de fuentes

**Desventajas:**
- Puede no incluir todos los art√≠culos WSJ
- Diferentes sesgos de cobertura
- Costos potenciales de API

---

## üìä Impacto en Resultados del Estudio

### An√°lisis Afectados:

1. **An√°lisis de Sentimientos (Moderado)**
   - ‚ö†Ô∏è Sentimientos sesgados hacia 2016-2020
   - ‚ö†Ô∏è Menor muestra para validar tendencias recientes
   - ‚úÖ Resultados v√°lidos para per√≠odo con datos

2. **Granger Causality (Alto)**
   - ‚ùå Desbalance temporal puede sesgar tests
   - ‚ö†Ô∏è Lag optimization menos confiable
   - ‚ö†Ô∏è P-values pueden ser artificialmente altos

3. **Modelo LSTM (Moderado)**
   - ‚ö†Ô∏è Entrenamiento sesgado hacia per√≠odo antiguo
   - ‚ö†Ô∏è Predicciones para 2024-2025 menos confiables
   - ‚úÖ Arquitectura y metodolog√≠a siguen siendo v√°lidas

4. **Detecci√≥n de Outliers (Bajo)**
   - ‚úÖ Precios del oro no afectados
   - ‚ö†Ô∏è Asociaci√≥n outliers-noticias menos representativa
   - ‚ö†Ô∏è Ventana ¬±3 d√≠as menos efectiva para a√±os recientes

---

## üéì Lecciones Aprendidas

### Para Futuros Proyectos de Web Scraping:

1. **Validaci√≥n continua:**
   - Implementar checks de volumen mensual
   - Alertas autom√°ticas cuando ca√≠da >20%
   - Logs detallados de tipos de URLs capturadas

2. **Testing robusto:**
   - Unit tests para patrones de extracci√≥n
   - Validaci√≥n de estructura HTML
   - Muestras aleatorias peri√≥dicas

3. **Documentaci√≥n:**
   - Registrar cambios en c√≥digo de scraping
   - Versionar scripts con fechas
   - Mantener historial de modificaciones

4. **Backup y auditor√≠a:**
   - Guardar snapshots mensuales
   - Comparaciones per√≠odo a per√≠odo
   - Detecci√≥n temprana de anomal√≠as

---

## üìû Contacto y Seguimiento

**Responsable:** [Asignar responsable]  
**Prioridad:** Alta  
**Fecha l√≠mite:** [Definir seg√∫n opci√≥n elegida]

**Pr√≥ximos pasos:**
1. [ ] Decidir qu√© opci√≥n implementar (1, 2, o 3)
2. [ ] Asignar recursos y tiempo
3. [ ] Iniciar implementaci√≥n
4. [ ] Actualizar resultados seg√∫n nuevos datos

---

## üìö Referencias

- **Notebook de an√°lisis:** `pruebas_observaciones/Analisis_Volumen_Noticias.ipynb`
- **Limitaciones documentadas:** `pruebas_observaciones/LIMITACION_DATOS_NOTICIAS.md`
- **Dataset original:** `hipervinculos_wsj.csv` (294,610 registros)
- **Dataset procesado:** `articulos_filtrados_ordenados.csv` (188,469 art√≠culos)
- **Dataset final:** `noticias_oro_limpias.csv` (13,434 noticias)

---

**√öltima actualizaci√≥n:** 2 de diciembre de 2025
